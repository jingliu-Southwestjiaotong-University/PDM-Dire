import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from skopt import BayesSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# 1. Load training data (no column names)
train_data = pd.read_excel('MIX_SS.xlsx', header=None)

# 2. Data preparation
# Assume the 1st column is the target variable,
# and columns 2 to 8 are the input features
X_train_raw = train_data.iloc[:, 1:8].copy()   # Feature matrix
y_train = train_data.iloc[:, 0].copy()         # Target variable

# 3. Data inspection and cleaning
# Identify numeric feature columns
numeric_columns = X_train_raw.select_dtypes(include=[np.number]).columns

# Compute training-set statistics (mean values)
train_mean = (
    X_train_raw[numeric_columns]
    .replace([np.inf, -np.inf], np.nan)
    .mean()
)

# Initialize the scaler
scaler = MinMaxScaler()

# 4. Preprocess training data
X_train = X_train_raw.copy()

# Replace infinite values with NaN
X_train.loc[:, numeric_columns] = (
    X_train[numeric_columns]
    .replace([np.inf, -np.inf], np.nan)
)

# Fill missing values using training-set means
X_train.fillna(train_mean, inplace=True)

# Normalize features
X_train_scaled = scaler.fit_transform(X_train)

# 5. Split data into training and testing subsets
X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(
    X_train_scaled,
    y_train,
    test_size=0.3,
    random_state=42
)

# 6. Model training and Bayesian hyperparameter optimization
param_space = {
    'n_estimators': (200, 500),
    'max_depth': (5, 30),
    'min_samples_split': (2, 10),
    'min_samples_leaf': (1, 10),
    'max_features': ['sqrt', 'log2']
}

model = RandomForestRegressor(random_state=42)

opt = BayesSearchCV(
    estimator=model,
    search_spaces=param_space,
    n_iter=30,
    cv=10,
    scoring='neg_mean_squared_error',
    random_state=42
)

opt.fit(X_train_final, y_train_final)

# 7. Model evaluation
print("Optimal hyperparameters:", opt.best_params_)

y_pred_test = opt.predict(X_test_final)
print("Test-set MSE:", mean_squared_error(y_test_final, y_pred_test))
print("Test-set R²:", r2_score(y_test_final, y_pred_test))

# ------------------- Prediction on new data -------------------
# 8. Load prediction data (no column names)
pred_data = pd.read_excel('prediction.xlsx', header=None)

# 9. Select the same feature columns as used in training
# Prediction data contains only features (columns 1–7)
X_pred_raw = pred_data.iloc[:, 0:7].copy()

# 10. Preprocess prediction data
X_pred = X_pred_raw.copy()

# 10.1 Replace infinite values with NaN
X_pred = X_pred.replace([np.inf, -np.inf], np.nan)

# 10.2 Fill missing values using training-set means (aligned by position)
mean_for_pred = pd.Series(train_mean.values, index=X_pred.columns)
X_pred = X_pred.fillna(mean_for_pred)

# 10.3 Normalize using the scaler fitted on the training data
X_pred_scaled = scaler.transform(X_pred)

# 11. Generate predictions
y_predictions = opt.predict(X_pred_scaled)

# 12. Save prediction results
pred_data['Predicted_y'] = y_predictions
pred_data.to_excel('prediction_results.xlsx', index=False, header=False)

print("Prediction completed. Results have been saved to 'prediction_results.xlsx'.")
